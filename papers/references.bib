% LLM Prompting Guide - Bibliography
% BibTeX entries for all papers referenced in the guide

% ============================================
% FOUNDATIONAL TECHNIQUES (2022)
% ============================================

@article{wei2022chain,
  title={Chain-of-Thought Prompting Elicits Reasoning in Large Language Models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc and Zhou, Denny},
  journal={arXiv preprint arXiv:2201.11903},
  year={2022},
  url={https://arxiv.org/abs/2201.11903}
}

@article{kojima2022large,
  title={Large Language Models are Zero-Shot Reasoners},
  author={Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
  journal={arXiv preprint arXiv:2205.11916},
  year={2022},
  url={https://arxiv.org/abs/2205.11916}
}

@article{wang2022self,
  title={Self-Consistency Improves Chain of Thought Reasoning in Language Models},
  author={Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny},
  journal={arXiv preprint arXiv:2203.11171},
  year={2022},
  url={https://arxiv.org/abs/2203.11171}
}

@article{yao2022react,
  title={ReAct: Synergizing Reasoning and Acting in Language Models},
  author={Yao, Shunyu and Zhao, Jeffrey and Yu, Dian and Du, Nan and Shafran, Izhak and Narasimhan, Karthik and Cao, Yuan},
  journal={arXiv preprint arXiv:2210.03629},
  year={2022},
  url={https://arxiv.org/abs/2210.03629}
}

% ============================================
% REASONING FRAMEWORKS (2023)
% ============================================

@article{yao2023tree,
  title={Tree of Thoughts: Deliberate Problem Solving with Large Language Models},
  author={Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Thomas L and Cao, Yuan and Narasimhan, Karthik},
  journal={arXiv preprint arXiv:2305.10601},
  year={2023},
  url={https://arxiv.org/abs/2305.10601}
}

@article{wang2023plan,
  title={Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models},
  author={Wang, Lei and Xu, Wanyu and Lan, Yihuai and Hu, Zhiqiang and Lan, Yunshi and Lee, Roy Ka-Wei and Lim, Ee-Peng},
  journal={arXiv preprint arXiv:2305.04091},
  year={2023},
  url={https://arxiv.org/abs/2305.04091}
}

% ============================================
% REASONING MODELS (2025)
% ============================================

@article{deepseek2025r1,
  title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning},
  author={DeepSeek-AI},
  journal={arXiv preprint arXiv:2501.12948},
  year={2025},
  url={https://arxiv.org/abs/2501.12948}
}

@article{marjanovic2025thoughtology,
  title={DeepSeek-R1 Thoughtology: Let's think about LLM Reasoning},
  author={Marjanovi{\'c}, Sara Vera and others},
  journal={arXiv preprint arXiv:2504.07128},
  year={2025},
  url={https://arxiv.org/abs/2504.07128}
}

@article{ghosal2025overthinking,
  title={Does Thinking More always Help? Mirage of Test-Time Scaling in Reasoning Models},
  author={Ghosal, Soumya Suvra and others},
  journal={arXiv preprint arXiv:2506.04210},
  year={2025},
  url={https://arxiv.org/abs/2506.04210}
}

% ============================================
% AGENTIC & LONG-CONTEXT (2025)
% ============================================

@article{zhang2025rlm,
  title={Recursive Language Models},
  author={Zhang, Alex L. and Kraska, Tim and Khattab, Omar},
  journal={arXiv preprint arXiv:2512.24601},
  year={2025},
  url={https://arxiv.org/abs/2512.24601}
}

@article{zhang2025ace,
  title={Agentic Context Engineering: Evolving Contexts for Self-Improving Language Models},
  author={Zhang, Qizheng and others},
  journal={arXiv preprint arXiv:2510.04618},
  year={2025},
  url={https://arxiv.org/abs/2510.04618}
}

@article{xu2025amem,
  title={A-MEM: Agentic Memory for LLM Agents},
  author={Xu, Wujiang and Liang, Zujie and Mei, Kai and Gao, Hang and Tan, Juntao and Zhang, Yongfeng},
  journal={arXiv preprint arXiv:2502.12110},
  year={2025},
  url={https://arxiv.org/abs/2502.12110}
}

@article{modelnative2025survey,
  title={Beyond Pipelines: A Survey of the Paradigm Shift toward Model-Native Agentic AI},
  author={Various},
  journal={arXiv preprint arXiv:2510.16720},
  year={2025},
  url={https://arxiv.org/abs/2510.16720}
}

% ============================================
% COMPREHENSIVE SURVEYS
% ============================================

@article{schulhoff2024prompt,
  title={The Prompt Report: A Systematic Survey of Prompting Techniques},
  author={Schulhoff, Sander and others},
  journal={arXiv preprint arXiv:2406.06608},
  year={2024},
  url={https://arxiv.org/abs/2406.06608}
}

@article{sahoo2024systematic,
  title={A Systematic Survey of Prompt Engineering in Large Language Models: Techniques and Applications},
  author={Sahoo, Pranab and others},
  journal={arXiv preprint arXiv:2402.07927},
  year={2024},
  url={https://arxiv.org/abs/2402.07927}
}

@article{besta2024demystifying,
  title={Demystifying Chains, Trees, and Graphs of Thoughts},
  author={Besta, Maciej and others},
  journal={arXiv preprint arXiv:2401.14295},
  year={2024},
  url={https://arxiv.org/abs/2401.14295}
}
